{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Web scraping</h3>\n",
    "\n",
    "After analyzing the search for Pikabu, I noticed that there are at least 1.3M posts tagged \"Текст\". In a practical way, I found that from about 10/02/2013 the number of posts with the \"Text\" tag every day was more than 300. Therefore, I decided to collect search pages with the \"Текст\" tag with a time step of 1 day. The data link looked something like this: https://pikabu.ru/tag/Текст/?d=2100 (https://pikabu.ru/tag/%D0%A2%D0%B5%D0%BA%D1%81%D1%82?d=2100). The maximum number of articles per one search query is 1k, if we assume that 9 articles are displayed on one page (in fact, 10 posts, but the last one is advertising, we are not interested in it), then the maximum number of pages for pagination is 112. The parameter for pagination in this case &page = 1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from user_agent import generate_user_agent\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>Original date of start is 2100, but I wrote down this number to run the next scraping if necessary</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# START_DATE = 2100\n",
    "START_DATE = 3186\n",
    "END_DATE = 4860\n",
    "HEADERS = {'User-Agent': generate_user_agent(device_type=\"desktop\", os=('mac', 'linux'))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrappingCycle(page_link, file_name):\n",
    "    try:\n",
    "        page_response = requests.get(page_link, timeout=5, headers=HEADERS)\n",
    "        if page_response.status_code == 200:\n",
    "            with open(\"./pages/\" + file_name + \".html\", 'w', encoding='utf-8') as file:\n",
    "                file.write(page_response.text)\n",
    "        else:\n",
    "            print(\"page response status code is \" + str(page_response.status_code) + \" on page link: \" + str(page_link))\n",
    "    except requests.Timeout as e:\n",
    "        print(\"It is time to timeout error: \" + str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrappingMain():\n",
    "    for day in range(START_DATE, END_DATE):\n",
    "        for page in range(1,112):\n",
    "            scrappingCycle(\"https://pikabu.ru/tag/%D0%A2%D0%B5%D0%BA%D1%81%D1%82?d=\" + str(day) + \"&page=\" + str(page), str(day) + \"-\" + str(page))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Launches of scrappingMain (time, start data - end data)</h3>\n",
    "<ul>\n",
    "    <li>20029s 2100-2234</li>\n",
    "    <li>362.2s 2234-2236</li>\n",
    "    <li>13260s 2236-2316</li>\n",
    "    <li>2555.3s 2316-2332</li>\n",
    "    <li>6007.3s 2332-2371</li>\n",
    "    <li>3938.4s 2371-2398</li>\n",
    "    <li>307202.3s 2398-2646</li>\n",
    "    <li>79864.5s 2646-3186</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "scrappingMain()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.6 64-bit",
   "name": "python386jvsc74a57bd0aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "metadata": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}